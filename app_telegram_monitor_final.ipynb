{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d7888c-2616-4078-b08e-75618210ef29",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Writing to google sheet #########\n",
    "from __future__ import print_function\n",
    "\n",
    "import googleapiclient.discovery as discovery\n",
    "from httplib2 import Http\n",
    "from oauth2client import client\n",
    "from oauth2client import file\n",
    "from oauth2client import tools\n",
    "\n",
    "import pygsheets\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "\n",
    "#authorization\n",
    "gc = pygsheets.authorize(service_file= path_docs + 'credentials2.json')\n",
    "\n",
    "#open the google spreadsheet (where 'PY to Gsheet Test' is the name of my sheet)\n",
    "sh = gc.open('telegram monitoring')\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[0]\n",
    "\n",
    "#Get the data from the Sheet into python as DF\n",
    "read = wks.get_as_df()\n",
    "#Print the head of the datframe\n",
    "read.head()\n",
    "\n",
    "app_start = read.iloc[0]['Values']\n",
    "print(app_start)\n",
    "search_term = read.iloc[2]['Values']\n",
    "print(search_term)\n",
    "\n",
    "def slack_msg(message):    \n",
    "    title = (f\"telegram_updates\")\n",
    "    slack_data = {\n",
    "        \"username\": \"telegram_monitoring\",\n",
    "        \"icon_emoji\": \":satellite:\",\n",
    "        #\"channel\" : \"#somerandomcahnnel\",\n",
    "        \"attachments\": [\n",
    "            {\n",
    "                \"color\": \"#9733EE\",\n",
    "                \"fields\": [\n",
    "                    {\n",
    "                        \"title\": title,\n",
    "                        \"value\": message,\n",
    "                        \"short\": \"false\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    byte_length = str(sys.getsizeof(slack_data))\n",
    "    headers = {'Content-Type': \"application/json\", 'Content-Length': byte_length}\n",
    "    response = requests.post(url, data=json.dumps(slack_data), headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "\n",
    "if app_start == \"Yes\":\n",
    "\n",
    "    ####Loading all the telegram packages\n",
    "    import configparser\n",
    "    from telethon import TelegramClient\n",
    "    from telethon.errors import SessionPasswordNeededError\n",
    "    from telethon.sync import TelegramClient\n",
    "    from telethon import functions, types\n",
    "\n",
    "    from telethon.tl.functions.channels import JoinChannelRequest\n",
    "    from telethon.errors.rpcerrorlist import FloodWaitError\n",
    "    import asyncio\n",
    "\n",
    "    ##Laoding the other required packages\n",
    "    import re\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    #TEXT EXTRACTION FROM IMAGES\n",
    "    # Loading all the required packages\n",
    "    from matplotlib import pyplot as plt\n",
    "    from easyocr import Reader ###Pretrained text extraction model from an AI company JaidedAI\n",
    "    import argparse\n",
    "    import cv2\n",
    "    import glob\n",
    "    import numpy as np\n",
    "\n",
    "    from IPython.display import clear_output\n",
    "    import time \n",
    "\n",
    "    ###TEXT ANALYSIS NLP Packages\n",
    "    ## for data\n",
    "    import pandas as pd\n",
    "    import collections\n",
    "    import json\n",
    "    ## for plotting\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    #import wordcloud\n",
    "    ## for text processing\n",
    "    import re\n",
    "    import nltk\n",
    "    ## for language detection\n",
    "    import langdetect \n",
    "    ## for sentiment\n",
    "    #from textblob import TextBlob\n",
    "    ## for ner\n",
    "    #import spacy\n",
    "    ## for vectorizer\n",
    "    from sklearn import feature_extraction, manifold\n",
    "    ## for word embedding\n",
    "    #import gensim.downloader as gensim_api\n",
    "    ## for topic modeling\n",
    "    #import gensim\n",
    "    import os\n",
    "\n",
    "    import json\n",
    "    import asyncio\n",
    "    from datetime import date, datetime\n",
    "\n",
    "    from telethon import TelegramClient\n",
    "    from telethon.errors import SessionPasswordNeededError\n",
    "    from telethon.tl.functions.messages import (GetHistoryRequest)\n",
    "    from telethon.tl.types import (\n",
    "        PeerChannel\n",
    "    )\n",
    "\n",
    "    from IPython.display import clear_output\n",
    "    import time \n",
    "    import shutil\n",
    "\n",
    "    ###Enter telegram channels credentials to access the telegram API\n",
    "    api_id = \"\"\n",
    "    api_hash = ''\n",
    "    # use full phone number including + and country code\n",
    "    phone = ''\n",
    "    username = ''\n",
    "    \n",
    "    client = TelegramClient(username, api_id, api_hash)\n",
    "    result = 'global'\n",
    "\n",
    "    ###Default code to cut the long running functions\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    print(\"Welcome to the Telegram monitoring application\")\n",
    "    async def main():\n",
    "        await client.start()\n",
    "        search = search_term\n",
    "        global result\n",
    "\n",
    "        result = await client(functions.contacts.SearchRequest(\n",
    "            q=search,\n",
    "            limit=100\n",
    "            ))\n",
    "    async with client:\n",
    "        client.loop.run_until_complete(main())\n",
    "\n",
    "    ###Data Manipulation of the data obtained above\n",
    "\n",
    "    strr = result.stringify()\n",
    "    strr1 = strr.split(\"chats\")[1]\n",
    "    trs = re.findall('Channel(.*)', strr1)\n",
    "    a = [ False if 'Forbidden' in a else True for a in trs ]\n",
    "\n",
    "    title = re.findall('title=(.*),', strr)\n",
    "    channel_id = re.findall('channel_id=(.*)', strr)\n",
    "    chn_name = re.findall('username=(.*),', strr1)\n",
    "    chn_name1 = [x.replace(\"'\", \"\") for x in chn_name  ]\n",
    "    chn_name1 = [x.replace(\",\", \"\") for x in chn_name1  ]\n",
    "    \n",
    "    print(chn_name1)\n",
    "    import copy\n",
    "    chn_name2 = copy.deepcopy(chn_name1)\n",
    "    \n",
    "    print(chn_name2)\n",
    "\n",
    "    for x in range(0, len(a)):\n",
    "        if a[x] != True:\n",
    "            chn_name2.insert(x,'None')\n",
    "\n",
    "    dtf_c = pd.DataFrame(\n",
    "        {'title': title,\n",
    "         'channelid': channel_id,\n",
    "         'username' : chn_name2\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Channels found\" + \"\\n\" + \"\\n\")\n",
    "    print(dtf_c)        \n",
    "    \n",
    "    #open the google spreadsheet (where 'PY to Gsheet Test' is the name of my sheet)\n",
    "    sh = gc.open('telegram monitoring')\n",
    "\n",
    "    #select the first sheet \n",
    "    wks = sh[1]\n",
    "\n",
    "    #update the first sheet with df, starting at cell B2. \n",
    "    wks.set_dataframe(dtf_c,(1,1))\n",
    "    print(\"Joining the channels before scrapping\" + \"\\n\" )\n",
    "    \n",
    "    for x in range(0,len(chn_name1)):\n",
    "        if 'None' in chn_name1:\n",
    "            chn_name1.remove(\"None\")\n",
    "        #chn_name1\n",
    "\n",
    "    ####TO JOIN THE GROUPS ########\n",
    "    \n",
    "    #chn_name1 = [x.remove(\"None\") for x in chn_name1  ]\n",
    "    CHANNELS = chn_name1 # the channels you want to join\n",
    "    \n",
    "    print(CHANNELS)\n",
    "\n",
    "    async def main():\n",
    "        async with client:\n",
    "        #async with TelegramClient(username, api_id, api_hash) as client:\n",
    "            for channel in CHANNELS:\n",
    "                try:\n",
    "                    await client(JoinChannelRequest(channel))\n",
    "                    print(\"Joining channel  \" + channel)\n",
    "                except FloodWaitError as fwe:\n",
    "                    print(f'Waiting for {fwe}')\n",
    "                    await asyncio.sleep(delay=fwe.seconds)\n",
    "\n",
    "    asyncio.run(main())    \n",
    "\n",
    "    print(\"Channel searched and joined for the search term\")\n",
    "    print(\"\\n\" + \"Waiting for 30 seconds\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    channel_scrape = read.iloc[4]['Values']\n",
    "    print(channel_scrape)\n",
    "    channel_name = read.iloc[6]['Values']\n",
    "    print(channel_name)\n",
    "\n",
    "    if channel_scrape == 'Yes':\n",
    "        \n",
    "        source_path = ''\n",
    "\n",
    "        path = source_path + channel_name\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)    \n",
    "\n",
    "        ## Code to scrape the messages from a channel passed\n",
    "        # some functions to parse json date\n",
    "        class DateTimeEncoder(json.JSONEncoder):\n",
    "            def default(self, o):\n",
    "                if isinstance(o, datetime):\n",
    "                    return o.isoformat()\n",
    "\n",
    "                if isinstance(o, bytes):\n",
    "                    return list(o)\n",
    "\n",
    "                return json.JSONEncoder.default(self, o)\n",
    "\n",
    "        # Create the client and connect\n",
    "        async def main(phone):\n",
    "            #await client.start()\n",
    "            print(\"Client Created\")\n",
    "            # Ensure you're authorized\n",
    "            global my_channel\n",
    "            my_channel = channel_name\n",
    "            print(\"\\n\" + \"Scrapping message for the channel  \" + my_channel)\n",
    "\n",
    "            offset_id = 0\n",
    "            limit = 100\n",
    "            all_messages = []\n",
    "            total_messages = 0\n",
    "            total_count_limit = 0\n",
    "\n",
    "            while True:\n",
    "                print(\"Current Offset ID is:\", offset_id, \"; Total Messages:\", total_messages)\n",
    "                history = await client(GetHistoryRequest(\n",
    "                    peer=my_channel,\n",
    "                    offset_id=offset_id,\n",
    "                    offset_date=None,\n",
    "                    add_offset=0,\n",
    "                    limit=limit,\n",
    "                    max_id=0,\n",
    "                    min_id=0,\n",
    "                    hash=0\n",
    "                ))\n",
    "                if not history.messages:\n",
    "                    break\n",
    "                messages = history.messages\n",
    "                for message in messages:\n",
    "                    all_messages.append(message.to_dict())\n",
    "                offset_id = messages[len(messages) - 1].id\n",
    "                total_messages = len(all_messages)\n",
    "                if total_count_limit != 0 and total_messages >= total_count_limit:\n",
    "                    break\n",
    "\n",
    "            with open(channel_name + '.json', 'w') as outfile:\n",
    "                json.dump(all_messages, outfile, cls=DateTimeEncoder)\n",
    "\n",
    "        async with client:\n",
    "            client.loop.run_until_complete(main(phone))\n",
    "\n",
    "        ###Storing the scrapped  channel content in a variable\n",
    "        import pandas as pd\n",
    "        df_txt_msgs = pd.read_json(channel_name + '.json')\n",
    "        print(\"\\n\" + \"Downloading images from \"  + channel_name)\n",
    "\n",
    "        async def run():\n",
    "            await client.connect()\n",
    "            channel = await client.get_entity(channel_name)\n",
    "            i = 0\n",
    "            global path1\n",
    "\n",
    "            path1 = path + \"/images/\"\n",
    "            async for message in client.iter_messages(channel):\n",
    "                i = i + 1\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print(\"\\n\" +  \"Number of messages  \" + str(i))\n",
    "                await client.download_media(message, file = path1 )\n",
    "\n",
    "        client.loop.run_until_complete(run())    \n",
    "\n",
    "        print(\"\\n\" + \"Image to text scraping started\" + \"\\n\")\n",
    "\n",
    "        langs = ['en']\n",
    "        reader = Reader(langs, gpu=0)\n",
    "\n",
    "        ###Reading text from Image ####\n",
    "        def plt_imshow(title, image):\n",
    "            # convert the image frame BGR to RGB color space and display it\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(image)\n",
    "            plt.title(title)\n",
    "            plt.grid(False)\n",
    "            plt.show()\n",
    "\n",
    "        def cleanup_text(text):\n",
    "            # strip out non-ASCII text so we can draw the text on the image\n",
    "            # using OpenCV\n",
    "            return \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "\n",
    "        # load the input image from disk\n",
    "        cnt = 0\n",
    "        img_nme = []\n",
    "        pix_perc = []\n",
    "\n",
    "        txtt = []\n",
    "        na_i = []\n",
    "        a = 1\n",
    "        pr1 = 0\n",
    "        \n",
    "        import os\n",
    "        listt = os.listdir(path) # dir is your directory path\n",
    "        number_files = len(listt)\n",
    "        \n",
    "        print(number_files)\n",
    "        if number_files > 1:\n",
    "            # iterate through list of files in a directory\n",
    "            for images in glob.iglob(path1  + '*.jpg'):\n",
    "                try:\n",
    "                    txt = []\n",
    "                    # load the input image from disk\n",
    "                    image = cv2.imread(images)\n",
    "                    img_name = images\n",
    "\n",
    "                    # OCR the input image using EasyOCR\n",
    "                    print(\"\\n\" + \"Extracting text from image...\" + str(a))\n",
    "\n",
    "                    #reader = Reader(langs, gpu=0)\n",
    "                    results = reader.readtext(image)\n",
    "                    ######Calculating the text pixels in the image\n",
    "                    if(len(results) != 0):\n",
    "                        #Capturing the results values in 3 different variables - bbox: Pixel values, text : Actual text content, Prob : Probability\n",
    "                        for (bbox, text, prob) in results:\n",
    "                            # cleanup the text and draw the box surrounding the text along\n",
    "                            # with the OCR'd text itself\n",
    "                            text = cleanup_text(text)\n",
    "                            txt.append(text)\n",
    "\n",
    "                    txtt.append(txt)\n",
    "                    images = images.replace(path + \"/images/\",\"\")\n",
    "                    img_nme.append(images)\n",
    "\n",
    "                    a = a+1\n",
    "                    global dtf\n",
    "\n",
    "                    dtf = pd.DataFrame(list(zip(img_nme,  txtt )), \n",
    "                        columns =['Image',  'text']) \n",
    "\n",
    "                    #dtf.to_excel(path + '/'+ 'image_extraction_data.xlsx')\n",
    "                    #print(dtf)\n",
    "                except:\n",
    "                    na_i.append(images)\n",
    "\n",
    "            print(\"\\n\" + \"Text scrapping completed\" + \"\\n\")\n",
    "            print(\"\\n\" + \"Text Analysis started\" + \"\\n\")\n",
    "            \n",
    "            def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "                ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "                text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\n",
    "                ## Tokenize (convert from string to list)\n",
    "                lst_text = text.split()\n",
    "                ## remove Stopwords\n",
    "                if lst_stopwords is not None:\n",
    "                    lst_text = [word for word in lst_text if word not in \n",
    "                                lst_stopwords]\n",
    "\n",
    "                ## Stemming (remove -ing, -ly, ...)\n",
    "                if flg_stemm == True:\n",
    "                    lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "                ## Lemmatisation (convert the word into root word)\n",
    "                if flg_lemm == True:\n",
    "                    lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "                    lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "                ## back to string from list\n",
    "                text = \" \".join(lst_text)\n",
    "                return text\n",
    "            dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords = None))\n",
    "\n",
    "            ####Finding the count of following words\n",
    "            lst_words = []\n",
    "            ## count\n",
    "            lst_grams = [len(word.split(\" \")) for word in lst_words]\n",
    "            vectorizer = feature_extraction.text.CountVectorizer(\n",
    "                             vocabulary=lst_words, \n",
    "                             ngram_range=(min(lst_grams),max(lst_grams)))\n",
    "            dtf_X = pd.DataFrame(vectorizer.fit_transform(dtf[\"text_clean\"]).todense(), columns=lst_words)\n",
    "            ## add the new features as columns\n",
    "            dtf = pd.concat([dtf, dtf_X.set_index(dtf.index)], axis=1)\n",
    "            dtf.head()\n",
    "\n",
    "            dtf.to_excel(path + \"/\" + \"Image_text_output.xlsx\")\n",
    "            print(\"\\n\" + \"Text Analysis completed\" + \"\\n\")\n",
    "            print(\"Check for the output at\" + path)\n",
    "            \n",
    "            df_txt_msgs[\"text_clean\"] = df_txt_msgs[\"message\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords = None))\n",
    "\n",
    "            ####Finding the count of following words\n",
    "            lst_words = []\n",
    "            ## count\n",
    "            lst_grams = [len(word.split(\" \")) for word in lst_words]\n",
    "            vectorizer = feature_extraction.text.CountVectorizer(\n",
    "                             vocabulary=lst_words, \n",
    "                             ngram_range=(min(lst_grams),max(lst_grams)))\n",
    "            dtf_X = pd.DataFrame(vectorizer.fit_transform(df_txt_msgs[\"text_clean\"]).todense(), columns=lst_words)\n",
    "            ## add the new features as columns\n",
    "            df_txt_msgs = pd.concat([df_txt_msgs, dtf_X.set_index(df_txt_msgs.index)], axis=1)\n",
    "            df_txt_msgs.head()\n",
    "\n",
    "            #df_txt_msgs.to_excel(path + \"/\" + \"Text_messages_output.xlsx\")\n",
    "            df_txt_msgs.to_csv(path + \"/\" + \"Text_messages_output.csv\")\n",
    "            \n",
    "            dtf_invoice = dtf.loc[dtf[''] == 1]\n",
    "            dtf_invoice_list = list(dtf_invoice['Image'])\n",
    "            dtf_invoice_list\n",
    "\n",
    "            \n",
    "            if not os.path.exists(path2):\n",
    "                os.makedirs(path2)    \n",
    "\n",
    "            os.chdir(path1)\n",
    "            for a in glob.iglob('*.jpg'):\n",
    "                if a in dtf_invoice_list:\n",
    "                    shutil.copy(a, path2)\n",
    "                    print(a)\n",
    "\n",
    "            print(\"\\n\" + \"Text Analysis completed\" + \"\\n\")\n",
    "            print(\"Check for the output at\" + path)\n",
    "\n",
    "        import shutil\n",
    "        shutil.make_archive(path , 'zip', path)\n",
    "\n",
    "        file_name = path   + \".zip\"\n",
    "        print(file_name)\n",
    "\n",
    "        import os\n",
    "        from slack_sdk import WebClient\n",
    "        from slack_sdk.errors import SlackApiError\n",
    "\n",
    "        SLACK_BOT_TOKEN_ = ''\n",
    "\n",
    "        client = WebClient(SLACK_BOT_TOKEN_)\n",
    "\n",
    "        try:\n",
    "            filepath=file_name\n",
    "            response = client.files_upload(channels='app-telegram-monitoring', file=file_name)\n",
    "            assert response[\"file\"]  # the uploaded file\n",
    "        except SlackApiError as e:\n",
    "            # You will get a SlackApiError if \"ok\" is False\n",
    "            assert e.response[\"ok\"] is False\n",
    "            assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "            print(f\"Got an error: {e.response['error']}\")    \n",
    "\n",
    "    else:\n",
    "        print(\"Scrapping turned off\")\n",
    "\n",
    "else:\n",
    "    print(\"Telegram App turned off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adf465-e093-48b7-8f1f-cdb2833f69a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
